#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üî• Êú¨Áï™Áí∞Â¢ÉHTMLÊßãÈÄ†ÂàÜÊûê
Êú¨Áï™Áí∞Â¢É„ÅßÂÆüÈöõ„ÅÆÂïèÈ°åÂÜÖÂÆπÊäΩÂá∫Â§±Êïó„ÅÆÂéüÂõ†Ë™øÊüª

ÁõÆÁöÑ:
1. ÂÆüÈöõ„ÅÆHTML„É¨„Çπ„Éù„É≥„ÇπÊßãÈÄ†„ÇíË©≥Á¥∞ÂàÜÊûê
2. ÂïèÈ°åÊñá„ÅåÂãïÁöÑË™≠„ÅøËæº„Åø„ÅãÈùôÁöÑË°®Á§∫„Åã„ÇíÁ¢∫Ë™ç
3. ÈÅ©Âàá„Å™ÊäΩÂá∫ÊâãÊ≥ï„ÇíÁâπÂÆö

ÂØæË±°: https://rccm-quiz-2025.onrender.com
"""

import requests
import json
import time
from datetime import datetime
import re
import urllib.parse
from typing import Dict, List, Optional, Tuple
import logging
# BeautifulSoup‰ΩøÁî®„Å™„Åó„Éê„Éº„Ç∏„Éß„É≥
import base64

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProductionHtmlStructureAnalysis:
    def __init__(self):
        self.base_url = "https://rccm-quiz-2025.onrender.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # ÂàÜÊûêÁµêÊûú
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'analysis_type': 'PRODUCTION_HTML_STRUCTURE_ANALYSIS',
            'target_url': self.base_url,
            'html_samples': {},
            'extraction_methods_tested': {},
            'dynamic_content_indicators': [],
            'static_content_found': [],
            'recommended_extraction_approach': '',
            'critical_findings': []
        }

    def capture_html_samples(self) -> Dict:
        """Êú¨Áï™Áí∞Â¢É„Åã„Çâ„ÅÆHTML„Çµ„É≥„Éó„É´ÂèñÂæó"""
        
        print("üîç Êú¨Áï™Áí∞Â¢ÉHTMLÊßãÈÄ†Ëß£ÊûêÈñãÂßã")
        print("=" * 80)
        
        html_samples = {}
        
        try:
            # 1. „Éõ„Éº„É†„Éö„Éº„Ç∏„ÅÆHTMLÊßãÈÄ†
            print("üìç Step 1: „Éõ„Éº„É†„Éö„Éº„Ç∏HTMLÊßãÈÄ†ÂàÜÊûê")
            home_response = self.session.get(self.base_url, timeout=30)
            
            if home_response.status_code == 200:
                html_samples['homepage'] = {
                    'url': self.base_url,
                    'status_code': home_response.status_code,
                    'content_length': len(home_response.text),
                    'raw_html': home_response.text[:5000],  # ÊúÄÂàù„ÅÆ5000ÊñáÂ≠ó
                    'encoding': home_response.encoding,
                    'headers': dict(home_response.headers)
                }
                print(f"  ‚úÖ „Éõ„Éº„É†„Éö„Éº„Ç∏ÂèñÂæóÊàêÂäü (Èï∑„Åï: {len(home_response.text)}ÊñáÂ≠ó)")
            
            time.sleep(2)
            
            # 2. Âü∫Á§éÁßëÁõÆË©¶È®ì„ÅÆHTMLÊßãÈÄ†
            print("üìç Step 2: Âü∫Á§éÁßëÁõÆË©¶È®ìHTMLÊßãÈÄ†ÂàÜÊûê")
            basic_url = f"{self.base_url}/start_exam/basic"
            basic_data = {'questions': '10'}
            
            basic_response = self.session.post(basic_url, data=basic_data, timeout=30, allow_redirects=True)
            
            if basic_response.status_code in [200, 302]:
                html_samples['basic_exam'] = {
                    'url': basic_url,
                    'data': basic_data,
                    'status_code': basic_response.status_code,
                    'content_length': len(basic_response.text),
                    'raw_html': basic_response.text[:10000],  # ÊúÄÂàù„ÅÆ10000ÊñáÂ≠ó
                    'full_html': basic_response.text,  # ÂÆåÂÖ®„Å™HTML
                    'encoding': basic_response.encoding,
                    'headers': dict(basic_response.headers),
                    'redirect_history': [r.url for r in basic_response.history]
                }
                print(f"  ‚úÖ Âü∫Á§éÁßëÁõÆË©¶È®ìÂèñÂæóÊàêÂäü (Èï∑„Åï: {len(basic_response.text)}ÊñáÂ≠ó)")
            
            time.sleep(2)
            
            # 3. Â∞ÇÈñÄÁßëÁõÆË©¶È®ì„ÅÆHTMLÊßãÈÄ† 
            print("üìç Step 3: Â∞ÇÈñÄÁßëÁõÆË©¶È®ìHTMLÊßãÈÄ†ÂàÜÊûê")
            specialist_url = f"{self.base_url}/start_exam/specialist"
            specialist_data = {
                'questions': '10',
                'category': 'Âª∫Ë®≠Áí∞Â¢É',
                'year': '2019'
            }
            
            specialist_response = self.session.post(specialist_url, data=specialist_data, timeout=30, allow_redirects=True)
            
            if specialist_response.status_code in [200, 302]:
                html_samples['specialist_exam'] = {
                    'url': specialist_url,
                    'data': specialist_data,
                    'status_code': specialist_response.status_code,
                    'content_length': len(specialist_response.text),
                    'raw_html': specialist_response.text[:10000],  # ÊúÄÂàù„ÅÆ10000ÊñáÂ≠ó
                    'full_html': specialist_response.text,  # ÂÆåÂÖ®„Å™HTML
                    'encoding': specialist_response.encoding,
                    'headers': dict(specialist_response.headers),
                    'redirect_history': [r.url for r in specialist_response.history]
                }
                print(f"  ‚úÖ Â∞ÇÈñÄÁßëÁõÆË©¶È®ìÂèñÂæóÊàêÂäü (Èï∑„Åï: {len(specialist_response.text)}ÊñáÂ≠ó)")
            
        except Exception as e:
            print(f"üí• HTMLÂèñÂæó„Ç®„É©„Éº: {str(e)}")
            html_samples['error'] = str(e)
        
        return html_samples

    def analyze_html_structure_deep(self, html_content: str, content_name: str) -> Dict:
        """HTMLÊßãÈÄ†„ÅÆË©≥Á¥∞ÂàÜÊûê"""
        
        print(f"üî¨ {content_name} Ë©≥Á¥∞ÊßãÈÄ†ÂàÜÊûê")
        
        analysis = {
            'content_name': content_name,
            'basic_stats': {},
            'dom_structure': {},
            'javascript_analysis': {},
            'form_analysis': {},
            'question_content_analysis': {},
            'dynamic_indicators': [],
            'extraction_recommendations': []
        }
        
        try:
            # Âü∫Êú¨Áµ±Ë®à
            analysis['basic_stats'] = {
                'total_length': len(html_content),
                'line_count': html_content.count('\n'),
                'tag_count': html_content.count('<'),
                'script_tags': html_content.count('<script'),
                'form_tags': html_content.count('<form'),
                'div_tags': html_content.count('<div'),
                'encoding_detected': 'utf-8' if 'utf-8' in html_content.lower() else 'unknown'
            }
            
            # Ê≠£Ë¶èË°®Áèæ„Åß„ÅÆDOMÂàÜÊûê
            try:
                # HTML„Çø„Ç∞„ÅÆÂü∫Êú¨„Ç´„Ç¶„É≥„Éà
                title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
                title = title_match.group(1) if title_match else 'No title'
                
                analysis['dom_structure'] = {
                    'title': title.strip(),
                    'meta_tags': len(re.findall(r'<meta[^>]*>', html_content, re.IGNORECASE)),
                    'script_tags': len(re.findall(r'<script[^>]*>', html_content, re.IGNORECASE)),
                    'form_tags': len(re.findall(r'<form[^>]*>', html_content, re.IGNORECASE)),
                    'div_tags': len(re.findall(r'<div[^>]*>', html_content, re.IGNORECASE)),
                    'p_tags': len(re.findall(r'<p[^>]*>', html_content, re.IGNORECASE)),
                    'span_tags': len(re.findall(r'<span[^>]*>', html_content, re.IGNORECASE)),
                    'input_tags': len(re.findall(r'<input[^>]*>', html_content, re.IGNORECASE)),
                    'button_tags': len(re.findall(r'<button[^>]*>', html_content, re.IGNORECASE))
                }
                
                # JavaScript„ÅÆÂàÜÊûê
                script_blocks = re.findall(r'<script[^>]*>(.*?)</script>', html_content, re.IGNORECASE | re.DOTALL)
                external_scripts = re.findall(r'<script[^>]*src=', html_content, re.IGNORECASE)
                
                js_analysis = {
                    'script_count': len(script_blocks) + len(external_scripts),
                    'external_scripts': len(external_scripts),
                    'inline_scripts': len(script_blocks),
                    'ajax_indicators': 0,
                    'fetch_indicators': 0,
                    'jquery_usage': False,
                    'dynamic_content_loading': False
                }
                
                for script_content in script_blocks:
                    script_lower = script_content.lower()
                    if 'ajax' in script_lower or 'xmlhttprequest' in script_lower:
                        js_analysis['ajax_indicators'] += 1
                    if 'fetch(' in script_lower:
                        js_analysis['fetch_indicators'] += 1
                    if 'jquery' in script_lower or '$(' in script_lower:
                        js_analysis['jquery_usage'] = True
                    if any(indicator in script_lower for indicator in ['load', 'onload', 'domcontentloaded', 'ready']):
                        js_analysis['dynamic_content_loading'] = True
                
                analysis['javascript_analysis'] = js_analysis
                
                # „Éï„Ç©„Éº„É†ÂàÜÊûê
                form_matches = re.findall(r'<form[^>]*>(.*?)</form>', html_content, re.IGNORECASE | re.DOTALL)
                form_analysis = {
                    'form_count': len(form_matches),
                    'forms_details': []
                }
                
                for form_content in form_matches:
                    action_match = re.search(r'action=["\']([^"\']*)["\']', form_content, re.IGNORECASE)
                    method_match = re.search(r'method=["\']([^"\']*)["\']', form_content, re.IGNORECASE)
                    
                    form_detail = {
                        'action': action_match.group(1) if action_match else '',
                        'method': method_match.group(1) if method_match else '',
                        'inputs': len(re.findall(r'<input[^>]*>', form_content, re.IGNORECASE)),
                        'buttons': len(re.findall(r'<button[^>]*>', form_content, re.IGNORECASE)),
                        'has_submit': bool(re.search(r'type=["\']submit["\']', form_content, re.IGNORECASE))
                    }
                    form_analysis['forms_details'].append(form_detail)
                
                analysis['form_analysis'] = form_analysis
                
            except Exception as e:
                analysis['dom_structure']['error'] = str(e)
            
            # ÂïèÈ°å„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÁõ¥Êé•Ê§úÁ¥¢
            question_patterns = [
                r'ÂïèÈ°å\s*\d+',
                r'Âïè\s*\d+',
                r'Question\s*\d+',
                r'[‚ë†‚ë°‚ë¢‚ë£‚ë§‚ë•‚ë¶‚ëß‚ë®‚ë©]',
                r'ÈÅ∏ÊäûËÇ¢',
                r'Á≠î„Åà',
                r'Ëß£Á≠î'
            ]
            
            question_content = {
                'pattern_matches': {},
                'visible_text_analysis': {},
                'potential_question_blocks': []
            }
            
            for pattern in question_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                question_content['pattern_matches'][pattern] = len(matches)
            
            # Ë¶ã„Åà„Çã„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊäΩÂá∫
            try:
                # „Çπ„ÇØ„É™„Éó„Éà„Å®„Çπ„Çø„Ç§„É´„Çø„Ç∞„ÇíÈô§Âéª
                clean_html = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.IGNORECASE | re.DOTALL)
                clean_html = re.sub(r'<style[^>]*>.*?</style>', '', clean_html, flags=re.IGNORECASE | re.DOTALL)
                
                # HTML„Çø„Ç∞„ÇíÈô§Âéª„Åó„Å¶„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÅøÊäΩÂá∫
                visible_text = re.sub(r'<[^>]+>', ' ', clean_html)
                visible_text = re.sub(r'\s+', ' ', visible_text).strip()  # Á©∫ÁôΩ„ÅÆÊ≠£Ë¶èÂåñ
                
                question_content['visible_text_analysis'] = {
                    'total_visible_text_length': len(visible_text),
                    'visible_text_sample': visible_text[:1000],
                    'contains_japanese': bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', visible_text)),
                    'question_keywords_found': []
                }
                
                # ÂïèÈ°åÈñ¢ÈÄ£„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÊ§úÁ¥¢
                question_keywords = ['ÂïèÈ°å', 'Âïè', 'ÈÅ∏Êäû', 'ÂõûÁ≠î', 'Ëß£Á≠î', 'Ê¨°„ÅÆ', '„Å´„Å§„ÅÑ„Å¶', '„Åß„ÅÇ„Çã', '„ÇÇ„ÅÆ„ÅØ']
                for keyword in question_keywords:
                    if keyword in visible_text:
                        question_content['visible_text_analysis']['question_keywords_found'].append(keyword)
                
            except Exception as e:
                question_content['visible_text_analysis']['error'] = str(e)
            
            analysis['question_content_analysis'] = question_content
            
            # ÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÊåáÊ®ô
            dynamic_indicators = []
            
            if js_analysis.get('ajax_indicators', 0) > 0:
                dynamic_indicators.append('AjaxÂëº„Å≥Âá∫„Åó„ÅÆÂ≠òÂú®')
            if js_analysis.get('fetch_indicators', 0) > 0:
                dynamic_indicators.append('Fetch API‰ΩøÁî®„ÅÆÂ≠òÂú®')
            if js_analysis.get('dynamic_content_loading'):
                dynamic_indicators.append('ÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑË™≠„ÅøËæº„Åø„ÅÆÂ≠òÂú®')
            if 'onload' in html_content.lower():
                dynamic_indicators.append('onload„Ç§„Éô„É≥„Éà„ÅÆÂ≠òÂú®')
            
            analysis['dynamic_indicators'] = dynamic_indicators
            
            # ÊäΩÂá∫Êé®Â•®ÊâãÊ≥ï
            recommendations = []
            
            if len(dynamic_indicators) > 0:
                recommendations.append('„Éò„ÉÉ„Éâ„É¨„Çπ„Éñ„É©„Ç¶„Ç∂(Selenium/Playwright)„ÅÆ‰ΩøÁî®„ÇíÊé®Â•®')
                recommendations.append('JavaScriptÂÆüË°åÂæå„ÅÆDOMÂèñÂæó„ÅåÂøÖË¶Å')
            else:
                recommendations.append('ÈùôÁöÑHTMLËß£Êûê„ÅßÂçÅÂàÜ')
                recommendations.append('BeautifulSoup + Ê≠£Ë¶èË°®Áèæ„ÅßÊäΩÂá∫ÂèØËÉΩ')
            
            if question_content['visible_text_analysis'].get('contains_japanese'):
                recommendations.append('Êó•Êú¨Ë™ûÊñáÂ≠ó„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞ÂØæÂøúÂøÖÈ†à')
            
            analysis['extraction_recommendations'] = recommendations
            
        except Exception as e:
            analysis['error'] = str(e)
        
        return analysis

    def save_html_samples(self, html_samples: Dict):
        """HTML„Çµ„É≥„Éó„É´„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò"""
        
        for sample_name, sample_data in html_samples.items():
            if 'full_html' in sample_data:
                filename = f"production_html_sample_{sample_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                
                try:
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(sample_data['full_html'])
                    print(f"  üìÑ HTML„Çµ„É≥„Éó„É´‰øùÂ≠ò: {filename}")
                except Exception as e:
                    print(f"  ‚ùå HTML„Çµ„É≥„Éó„É´‰øùÂ≠òÂ§±Êïó {sample_name}: {str(e)}")

    def run_comprehensive_html_analysis(self):
        """ÂåÖÊã¨ÁöÑHTMLÊßãÈÄ†ÂàÜÊûê„ÅÆÂÆüË°å"""
        
        print("üî• Êú¨Áï™Áí∞Â¢ÉHTMLÊßãÈÄ†ÂàÜÊûêÈñãÂßã")
        print("=" * 80)
        print("ÁõÆÁöÑ: ÂïèÈ°åÂÜÖÂÆπÊäΩÂá∫Â§±ÊïóÂéüÂõ†„ÅÆÁâπÂÆö")
        print("ÂØæË±°: https://rccm-quiz-2025.onrender.com")
        print("=" * 80)
        
        start_time = time.time()
        
        # Step 1: HTML„Çµ„É≥„Éó„É´ÂèñÂæó
        html_samples = self.capture_html_samples()
        self.analysis_results['html_samples'] = html_samples
        
        # Step 2: ÂêÑ„Çµ„É≥„Éó„É´„ÅÆË©≥Á¥∞ÂàÜÊûê
        extraction_methods = {}
        
        for sample_name, sample_data in html_samples.items():
            if 'full_html' in sample_data:
                print(f"\nüî¨ {sample_name} Ë©≥Á¥∞ÂàÜÊûê")
                print("-" * 60)
                
                analysis = self.analyze_html_structure_deep(sample_data['full_html'], sample_name)
                extraction_methods[sample_name] = analysis
                
                # ÈáçË¶Å„Å™Áô∫Ë¶ã‰∫ãÈ†Ö„ÅÆË°®Á§∫
                if analysis.get('dynamic_indicators'):
                    print(f"  üö® ÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑÊåáÊ®ô: {len(analysis['dynamic_indicators'])}‰ª∂")
                    for indicator in analysis['dynamic_indicators']:
                        print(f"    ‚Ä¢ {indicator}")
                
                if analysis.get('extraction_recommendations'):
                    print(f"  üí° ÊäΩÂá∫Êé®Â•®ÊâãÊ≥ï:")
                    for rec in analysis['extraction_recommendations']:
                        print(f"    ‚Ä¢ {rec}")
                
                question_analysis = analysis.get('question_content_analysis', {})
                visible_analysis = question_analysis.get('visible_text_analysis', {})
                if visible_analysis.get('question_keywords_found'):
                    print(f"  ‚úÖ ÂïèÈ°åÈñ¢ÈÄ£„Ç≠„Éº„ÉØ„Éº„ÉâÁô∫Ë¶ã: {visible_analysis['question_keywords_found']}")
                else:
                    print(f"  ‚ùå ÂïèÈ°åÈñ¢ÈÄ£„Ç≠„Éº„ÉØ„Éº„ÉâÊú™Áô∫Ë¶ã")
        
        self.analysis_results['extraction_methods_tested'] = extraction_methods
        
        # Step 3: HTML„Çµ„É≥„Éó„É´„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
        print(f"\nüíæ HTML„Çµ„É≥„Éó„É´‰øùÂ≠ò")
        print("-" * 60)
        self.save_html_samples(html_samples)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Step 4: Á∑èÂêàÂà§ÂÆö„Å®Êé®Â•®ÊâãÊ≥ï
        self.generate_analysis_report(duration)
        
        return self.analysis_results

    def generate_analysis_report(self, duration: float):
        """ÂàÜÊûê„É¨„Éù„Éº„ÉàÁîüÊàê"""
        
        print("\n" + "=" * 80)
        print("üî• Êú¨Áï™Áí∞Â¢ÉHTMLÊßãÈÄ†ÂàÜÊûêÁµêÊûú")
        print("=" * 80)
        
        extraction_methods = self.analysis_results['extraction_methods_tested']
        
        print(f"üìä ÂàÜÊûêÁµ±Ë®à:")
        print(f"  ÂàÜÊûêÂØæË±°„Çµ„É≥„Éó„É´: {len(extraction_methods)}")
        print(f"  ÂàÜÊûêÂÆüË°åÊôÇÈñì: {duration:.1f}Áßí")
        print()
        
        # ÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÂà§ÂÆö
        dynamic_content_found = False
        static_content_with_questions = False
        
        all_dynamic_indicators = []
        all_recommendations = []
        
        for sample_name, analysis in extraction_methods.items():
            dynamic_indicators = analysis.get('dynamic_indicators', [])
            if dynamic_indicators:
                dynamic_content_found = True
                all_dynamic_indicators.extend(dynamic_indicators)
            
            recommendations = analysis.get('extraction_recommendations', [])
            all_recommendations.extend(recommendations)
            
            question_analysis = analysis.get('question_content_analysis', {})
            visible_analysis = question_analysis.get('visible_text_analysis', {})
            if visible_analysis.get('question_keywords_found'):
                static_content_with_questions = True
        
        # ÈáçË¶Å„Å™Áô∫Ë¶ã‰∫ãÈ†Ö
        critical_findings = []
        
        if dynamic_content_found:
            critical_findings.append("ÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅÆÂ≠òÂú®„ÇíÁ¢∫Ë™ç")
            print("üö® ÈáçË¶ÅÁô∫Ë¶ã: ÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅåÊ§úÂá∫„Åï„Çå„Åæ„Åó„Åü")
            print("  ÂãïÁöÑÊåáÊ®ô:")
            for indicator in set(all_dynamic_indicators):
                print(f"    ‚Ä¢ {indicator}")
        
        if not static_content_with_questions:
            critical_findings.append("ÈùôÁöÑHTML„Å´ÂïèÈ°åÊñá„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å™„ÅÑ")
            print("üö® ÈáçË¶ÅÁô∫Ë¶ã: ÈùôÁöÑHTML„Å´ÂïèÈ°åÊñá„ÅåÁ¢∫Ë™ç„Åß„Åç„Åæ„Åõ„Çì")
        else:
            critical_findings.append("ÈùôÁöÑHTML„Å´ÂïèÈ°åÈñ¢ÈÄ£„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅåÂ≠òÂú®")
            print("‚úÖ Áô∫Ë¶ã: ÈùôÁöÑHTML„Å´ÂïèÈ°åÈñ¢ÈÄ£„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅåÂ≠òÂú®„Åó„Åæ„Åô")
        
        self.analysis_results['critical_findings'] = critical_findings
        
        print()
        
        # Êé®Â•®„Ç¢„Éó„É≠„Éº„ÉÅ
        if dynamic_content_found:
            recommended_approach = "„Éò„ÉÉ„Éâ„É¨„Çπ„Éñ„É©„Ç¶„Ç∂(Selenium/Playwright)„Å´„Çà„ÇãÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑÂèñÂæó"
            print("üí° Êé®Â•®„Ç¢„Éó„É≠„Éº„ÉÅ: „Éò„ÉÉ„Éâ„É¨„Çπ„Éñ„É©„Ç¶„Ç∂„ÅÆ‰ΩøÁî®")
            print("  ÁêÜÁî±: JavaScriptÂÆüË°å„Å´„Çà„ÇãÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑË™≠„ÅøËæº„Åø„ÅåÂøÖË¶Å")
        else:
            recommended_approach = "ÈùôÁöÑHTMLËß£Êûê(BeautifulSoup + Ê≠£Ë¶èË°®Áèæ)"
            print("üí° Êé®Â•®„Ç¢„Éó„É≠„Éº„ÉÅ: ÈùôÁöÑHTMLËß£Êûê")
            print("  ÁêÜÁî±: ÂãïÁöÑ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅåÊ§úÂá∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ")
        
        self.analysis_results['recommended_extraction_approach'] = recommended_approach
        
        print()
        
        # ÊúÄÁµÇÂà§ÂÆö
        if dynamic_content_found and not static_content_with_questions:
            print("üéØ ÁµêË´ñ: ÂïèÈ°åÂÜÖÂÆπ„ÅØÂãïÁöÑ„Å´Ë™≠„ÅøËæº„Åæ„Çå„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ")
            print("  ÂØæÁ≠ñ: „Éò„ÉÉ„Éâ„É¨„Çπ„Éñ„É©„Ç¶„Ç∂„Å´„Çà„ÇãÂÆü„Éñ„É©„Ç¶„Ç∂„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥„ÅåÂøÖË¶Å")
            credibility_status = "DYNAMIC_CONTENT_CONFIRMED"
        elif static_content_with_questions:
            print("üéØ ÁµêË´ñ: ÂïèÈ°åÂÜÖÂÆπ„ÅØÈùôÁöÑHTML„Å´Âê´„Åæ„Çå„Å¶„ÅÑ„Çã")
            print("  ÂØæÁ≠ñ: ÊäΩÂá∫ÊâãÊ≥ï„ÅÆÊîπÂñÑ„Å´„Çà„ÇäËß£Ê±∫ÂèØËÉΩ")
            credibility_status = "STATIC_CONTENT_CONFIRMED"
        else:
            print("üéØ ÁµêË´ñ: ËøΩÂä†Ë™øÊüª„ÅåÂøÖË¶Å")
            print("  ÂØæÁ≠ñ: „Çà„ÇäË©≥Á¥∞„Å™ÂàÜÊûê„Å®„ÉÜ„Çπ„ÉàÊâãÊ≥ï„ÅÆÊ§úË®é")
            credibility_status = "NEEDS_FURTHER_INVESTIGATION"
        
        # Ë©≥Á¥∞„É¨„Éù„Éº„Éà‰øùÂ≠ò
        report_filename = f"production_html_structure_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüìÑ Ë©≥Á¥∞ÂàÜÊûê„É¨„Éù„Éº„Éà: {report_filename}")
        print("\nüîí Êú¨Áï™Áí∞Â¢ÉHTMLÊßãÈÄ†ÂàÜÊûêÂÆå‰∫Ü")

def main():
    """„É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞"""
    print("üî• Êú¨Áï™Áí∞Â¢ÉHTMLÊßãÈÄ†ÂàÜÊûê")
    print("ÂïèÈ°åÂÜÖÂÆπÊäΩÂá∫Â§±ÊïóÂéüÂõ†„ÅÆÁâπÂÆö")
    print()
    
    analyzer = ProductionHtmlStructureAnalysis()
    results = analyzer.run_comprehensive_html_analysis()
    
    return results

if __name__ == "__main__":
    main()